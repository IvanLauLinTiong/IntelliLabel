{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041b4c8e",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d6f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import neattext.functions as nfx\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e9640",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64502c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels = [\n",
    "    'bug', \n",
    "    'enhancement', \n",
    "    'question'\n",
    "]\n",
    "\n",
    "reg_obj = re.compile(r'[^\\u0000-\\u007F]+', re.UNICODE)\n",
    "def is_english_text(text):\n",
    "    return (False if reg_obj.match(text) else True)\n",
    "\n",
    "# remove the stopwords, emojis from the text and convert it into lower case\n",
    "def neatify_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = nfx.remove_stopwords(text)\n",
    "    text = nfx.remove_emojis(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3828bc",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996cde82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/distil-bert-uncased-finetuned-github-issues/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2d5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b090b9c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c37c9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change your text here\n",
    "text = \"\"\"\n",
    "Keras load_image is not working if I display any image on the top of the page :(\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"请问可以加个text field吗？\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2827a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "    # strip away any \" \" and \\n or \\t\n",
    "    text = text.strip(\" \\n\\t\")\n",
    "    \n",
    "    if is_english_text(text):\n",
    "        text = neatify_text(text)\n",
    "        tokenized_sentence = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**tokenized_sentence)\n",
    "        predictions = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "        _, preds = torch.max(predictions, dim=-1)\n",
    "        predicted = labels[preds.item()]\n",
    "        print(f\"Predicted: {predicted}\")\n",
    "    else:\n",
    "        print(\"Sentence have to be in english language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14104ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: bug\n"
     ]
    }
   ],
   "source": [
    "inference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17e6fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on eval data\n",
    "test_data = pd.read_pickle(\"./dataset/eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e4b5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: correct daily tweet count value twitto started, daily automated tweet reports incomplete period. tweeted value \"number geolocated tweets 24 hours\" incorrect. instead scheduled right away, daily tweet scheduled published 24h period.\n",
      "\n",
      "Predicted: bug\n",
      "Actual: bug\n"
     ]
    }
   ],
   "source": [
    "# modify ith sample here\n",
    "i = 900\n",
    "\n",
    "X, actual = test_data.iloc[i]['descriptions'], test_data.iloc[i]['labels']\n",
    "print(f\"Descriptions: {X}\\n\")\n",
    "inference(X)\n",
    "print(f\"Actual: {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc1317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
